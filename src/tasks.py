"""Task definitions for the QA Automator project."""
from __future__ import annotations

from crewai import Task


def parse_inputs_task(agent, test_cases_path: str, tested_info_path: str) -> Task:
    description = f"""
    Load the Qase-exported test cases from `{test_cases_path}` and the already-tested
    metadata from `{tested_info_path}`.
    Build end-to-end scenarios by combining related test cases and save them into
    `scenarios.json`. Return only a compact summary plus the next single scenario
    to automate so the context stays small.
    """

    expected_output = (
        "A compact report with total/pending counts, path to scenarios.json, and one "
        "next scenario ready for automation."
    )

    return Task(
        name="Parse Qase inputs",
        description=description,
        expected_output=expected_output,
        agent=agent,
    )


def map_appflow_task(agent, artifacts_dir: str) -> Task:
    description = f"""
    Open `{artifacts_dir}/current_scenario.json` (generated by qase_parser) to learn which
    scenario is queued for automation next. Use qase_parser to fetch the same scenario id so
    you always share context with the QA Manager. For every case in that scenario, call
    `app_flow_memory`'s `suggest_context` to capture best starting screens, confidence, and
    rationale. Do not call `screen_inspector` during this upfront planning stage; this task runs
    before active Maestro execution and should rely on memory + scenario semantics only.
    Immediately persist each hypothesis via
    `app_flow_memory` `record_plan` so the memory file reflects that this run attempted to cover the
    case (even before automation).
    When the memory lacks data, derive a hypothesis from test titles and steps, label it as
    low-confidence, and explicitly request confirmation from the manager after the first
    automation attempt. Consolidate all findings into `{artifacts_dir}/appflow_plan_<scenario_id>.md`.
    The plan must include: scenario summary, recommended navigation order, per-case starting
    context, warnings about known flaky selectors (if any), confidence level, and gaps the
    manager should watch for. Keep it concise, but ensure each case has a checklist the manager
    can follow and a reminder to feed observations back into app_flow_memory.
    """

    expected_output = (
        "Markdown plan saved to artifacts/appflow_plan_<scenario_id>.md summarizing per-case "
        "entry points, risks, and memory-backed hints."
    )

    return Task(
        name="Draft AppFlow plan",
        description=description,
        expected_output=expected_output,
        agent=agent,
    )


def automate_tests_task(agent, app_path: str, artifacts_dir: str, max_attempts: int) -> Task:
    description = f"""
    Read exactly one pending scenario from qase_parser and automate only that scenario in
    this run (do not process multiple scenarios). You MUST automate all test cases inside
    this chosen scenario in the same run. For each case in this chosen scenario,
    synthesize a naive Maestro flow, run it against `{app_path}`, and capture artifacts under
    `{artifacts_dir}`. Always grab a screenshot whenever a run fails so you can inspect
    the UI state before the next iteration. If testcase selector/action text is not in English
    but the app UI is English, infer the real selector names from screenshot content and use
    those actual on-screen labels/IDs in Maestro commands. You MUST use guidance from
    `skills/maestro-test-writing/SKILL.md` while writing/fixing Maestro flow files.
    Before writing flow for each case, consult the latest AppFlow plan file inside
    `{artifacts_dir}` (generated by the AppFlow specialist) and, if a case is missing, call the
    specialist directly to obtain `recommended_start`. If the plan is low-confidence or incomplete,
    continue in discovery mode: run minimal probing flows to move forward screen-by-screen instead
    of blocking the whole scenario. After every attempt, report back to AppFlow:
    case id, scenario id, status, most likely current screen/location, and failure cause so
    AppFlow knowledge improves on each run.
    For every failed run, inspect returned `failure_context` (log excerpt, cause,
    recommendation, debug_context.ui_text_candidates, failed_selector) and rewrite the Maestro
    flow before retrying (do not repeat unchanged selectors/commands that already failed).
    After each flow writing/editing iteration (first draft and every retry), delegate the
    full YAML to MaestroSenior and apply returned corrections before calling maestro_cli.
    Treat MaestroSenior review as mandatory and mention in attempt logs what was fixed.
    Respect the attempt limit of {max_attempts} tries per case. For each attempt log inputs, Maestro stdout/stderr,
    and whether additional screenshots were requested. Mark unresolved cases as problematic.
    """

    expected_output = (
        "JSON summary saved to artifacts/automation_report.json that lists every test id, "
        "attempt count, final status (passed, failed, problematic), and artifact pointers."
    )

    return Task(
        name="Automate tests with Maestro",
        description=description,
        expected_output=expected_output,
        agent=agent,
    )


def summarize_results_task(agent) -> Task:
    description = (
        "Produce a final report that highlights: total tests, automated successfully, flagged as "
        "problematic, and actionable insights for manual QA. Include links to screenshots/logs."
    )

    expected_output = "Markdown report stored at artifacts/summary.md"

    return Task(
        name="Summarize automation run",
        description=description,
        expected_output=expected_output,
        agent=agent,
    )
